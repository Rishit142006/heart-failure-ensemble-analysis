# -*- coding: utf-8 -*-
"""CL-AI practical-10

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LjQl5EtPGV1Toh-svpiKB4H36OtWGm7e
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

from google.colab import files
uploaded = files.upload()

import pandas as pd
data = pd.read_csv(next(iter(uploaded)))
data.head()

X = data.drop('DEATH_EVENT', axis=1)
y = data['DEATH_EVENT']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Base models
lr = LogisticRegression(max_iter=1000)
svm = SVC(kernel='rbf', probability=True)
rf = RandomForestClassifier(n_estimators=100, random_state=42)
gb = GradientBoostingClassifier(random_state=42)

lr.fit(X_train_scaled, y_train)
svm.fit(X_train_scaled, y_train)
rf.fit(X_train, y_train)
gb.fit(X_train, y_train)

# Voting Ensemble
voting = VotingClassifier(
    estimators=[
        ('LogReg', lr),
        ('SVM', svm),
        ('RF', rf)
    ],
    voting='soft'
)
voting.fit(X_train_scaled, y_train)

# Predictions
models = {
    "Logistic Regression": lr.predict(X_test_scaled),
    "SVM": svm.predict(X_test_scaled),
    "Random Forest": rf.predict(X_test),
    "Gradient Boosting": gb.predict(X_test),
    "Voting Ensemble": voting.predict(X_test_scaled)
}

# Metrics
results = []
for name, preds in models.items():
    results.append([
        name,
        accuracy_score(y_test, preds),
        precision_score(y_test, preds),
        recall_score(y_test, preds),
        f1_score(y_test, preds)
    ])

results_df = pd.DataFrame(
    results,
    columns=["Model", "Accuracy", "Precision", "Recall", "F1-score"]
)

print("=== Ensemble & Base Model Comparison ===")
print(results_df)

# Feature Importance (Random Forest)
feature_importance = pd.DataFrame({
    "Feature": X.columns,
    "Importance": rf.feature_importances_
}).sort_values(by="Importance", ascending=False)

print("\nTop 10 Important Features:")
print(feature_importance.head(10))

# Confusion Matrix for Ensemble
print("\nConfusion Matrix (Voting Ensemble):")
print(confusion_matrix(y_test, voting.predict(X_test_scaled)))

import matplotlib.pyplot as plt
from sklearn.metrics import ConfusionMatrixDisplay

y_pred_ensemble = voting.predict(X_test_scaled)
cm = confusion_matrix(y_test, y_pred_ensemble)

disp = ConfusionMatrixDisplay(
    confusion_matrix=cm,
    display_labels=["Alive", "Death"]
)
plt.figure(figsize=(6, 5))
disp.plot()
plt.title("Confusion Matrix - Voting Ensemble")
plt.show()

print(data['DEATH_EVENT'].value_counts())
print("\nPercentage:")
print(data['DEATH_EVENT'].value_counts(normalize=True)*100)

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

models = {
    "Logistic Regression": lr,
    "SVM": svm,
    "Random Forest": rf,
    "Gradient Boosting": gb
}

plt.figure()

for name, model in models.items():
    if hasattr(model, "predict_proba"):
        probs = model.predict_proba(X_test_scaled)[:,1]
    else:
        from sklearn.calibration import CalibratedClassifierCV
        cal = CalibratedClassifierCV(model)
        cal.fit(X_train_scaled, y_train)
        probs = cal.predict_proba(X_test_scaled)[:,1]

    auc = roc_auc_score(y_test, probs)
    fpr, tpr, _ = roc_curve(y_test, probs)
    plt.plot(fpr, tpr, label=f"{name} (AUC={auc:.3f})")

plt.plot([0,1],[0,1],'--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve Comparison")
plt.legend()
plt.show()

#precision call curve

from sklearn.metrics import precision_recall_curve, average_precision_score

plt.figure()

for name, model in models.items():
    if hasattr(model, "predict_proba"):
        probs = model.predict_proba(X_test_scaled)[:,1]
    else:
        from sklearn.calibration import CalibratedClassifierCV
        cal = CalibratedClassifierCV(model)
        cal.fit(X_train_scaled, y_train)
        probs = cal.predict_proba(X_test_scaled)[:,1]

    precision, recall, _ = precision_recall_curve(y_test, probs)
    ap_score = average_precision_score(y_test, probs)
    plt.plot(recall, precision, label=f"{name} (AP={ap_score:.3f})")

plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precisionâ€“Recall Curves")
plt.legend()
plt.show()

import numpy as np

print("\n=== Random Forest Importance ===")
importances = pd.Series(rf.feature_importances_, index=X.columns)
print(importances.sort_values(ascending=False).head(10))

print("\n=== Gradient Boosting Importance ===")
gb_imp = pd.Series(gb.feature_importances_, index=X.columns)
print(gb_imp.sort_values(ascending=False).head(10))

print("\n=== Logistic Regression Coefficients ===")
lr_coef = pd.Series(np.abs(lr.coef_[0]), index=X.columns)
print(lr_coef.sort_values(ascending=False).head(10))

#Threshold tuning

import numpy as np
from sklearn.metrics import f1_score, recall_score

probs = lr.predict_proba(X_test_scaled)[:,1]
thresholds = np.arange(0.1, 0.9, 0.05)

print("Thresh  Recall   F1")
for t in thresholds:
    preds = (probs >= t).astype(int)
    print(f"{t:.2f}   {recall_score(y_test,preds):.3f}   {f1_score(y_test,preds):.3f}")

import joblib
joblib.dump(rf, "heart_failure_predictor.joblib")